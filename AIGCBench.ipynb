{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AkKTzK7fEtU","executionInfo":{"status":"ok","timestamp":1723523334923,"user_tz":300,"elapsed":1791,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"16487e7a-12c9-447f-b5a9-f44898fe6d3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'AIGCBench'...\n","remote: Enumerating objects: 65, done.\u001b[K\n","remote: Counting objects: 100% (65/65), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 65 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (65/65), 6.52 MiB | 12.76 MiB/s, done.\n","Resolving deltas: 100% (29/29), done.\n","/content/AIGCBench\n"]}],"source":["!git clone https://github.com/BenchCouncil/AIGCBench.git\n","%cd AIGCBench"]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio\n","!pip install transformers\n","!pip install opencv-python-headless\n","!pip install scikit-image"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpEs-I7TfT-z","executionInfo":{"status":"ok","timestamp":1721028813842,"user_tz":-330,"elapsed":77015,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"924075c0-d0dc-4fe7-a077-225e41219a33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.25.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.7.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.1)\n"]}]},{"cell_type":"code","source":["!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOXO4Qgdg95F","executionInfo":{"status":"ok","timestamp":1721028833613,"user_tz":-330,"elapsed":19776,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"6e6e96b0-87fa-488f-8af9-121905237c23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.2.0\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-84qm45c1\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-84qm45c1\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.0+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.5.82)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=63ef75a574a55990464a1d413b85e00d53711379f0a6182c9b010f2b847e8075\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-saxao5q_/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n","Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}]},{"cell_type":"code","source":["!mkdir -p models\n","!wget -O models/ViT-L-14.pt https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQS0WjX_fVP4","executionInfo":{"status":"ok","timestamp":1721028840252,"user_tz":-330,"elapsed":6653,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"09ad53ea-5eb1-4c5e-9ef7-8e0b095ca83d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-15 07:33:52--  https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\n","Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.59, 2620:1ec:bdf::59\n","Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.59|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 932768134 (890M) [application/octet-stream]\n","Saving to: ‘models/ViT-L-14.pt’\n","\n","models/ViT-L-14.pt  100%[===================>] 889.56M   141MB/s    in 6.6s    \n","\n","2024-07-15 07:33:59 (135 MB/s) - ‘models/ViT-L-14.pt’ saved [932768134/932768134]\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uBY7LGzfa8-","executionInfo":{"status":"ok","timestamp":1721028872152,"user_tz":-330,"elapsed":31903,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"ef56242f-9266-489f-ef6c-f7a1bb9da8e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!sed -i 's|path_to_dir|/content/AIGCBench/models|g' eval.py"],"metadata":{"id":"irxO1tjQfc-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The below cell is updating the compute_video_video_similarity method for smaller frames"],"metadata":{"id":"TrtbPG1gbTK7"}},{"cell_type":"code","source":["%%writefile /content/AIGCBench/eval.py\n","\n","import clip\n","from PIL import Image\n","import numpy as np\n","from einops import rearrange\n","from metrics.clip_score import calculate_clip_score\n","from utils import load_video, load_image\n","from skimage.metrics import mean_squared_error, structural_similarity as ssim\n","\n","model, preprocess = clip.load(\"/content/AIGCBench/models/ViT-L-14.pt\", device=\"cuda\")\n","\n","# GenVideo-RefVideo SSIM\n","def compute_video_video_similarity_ssim(reference_video_path,\n","                                        video_path,\n","                                        keyframes=[0, 4, 8, 12, 15]):\n","    global model, preprocess\n","\n","    reference_video = load_video(reference_video_path)\n","    try:\n","        video_frames = load_video(video_path,\n","                                  size=(reference_video[0].shape[1],\n","                                        reference_video[0].shape[0]))[:16]\n","    except:\n","        print(video_path, ' Error !!')\n","        return {'ssim': 0, 'state': 0}\n","\n","    all_frame_scores = 0.\n","    for idx, frames in enumerate(keyframes):\n","        im1 = np.array(\n","            reference_video[10 + frames * 3]\n","        )  # In webvid, we start with the tenth frame of the video for the motion. Different fps * 3\n","        im2 = np.array(video_frames[frames])\n","\n","        ssim_value = ssim(im1,\n","                          im2,\n","                          multichannel=True,\n","                          win_size=7,\n","                          channel_axis=-1)\n","\n","        all_frame_scores += ssim_value\n","\n","    all_frame_scores /= len(keyframes)\n","    return {'ssim': all_frame_scores, 'state': 1}\n","\n","# GenVideo clip\n","def compute_temporal_consistency(video_path):\n","    global model, preprocess\n","\n","    all_frame_scores = 0.\n","    video_frames = load_video(video_path)[:16]  # get pre 16 frames\n","    for i in range(1, len(video_frames)):\n","        pre_frame = Image.fromarray(video_frames[i - 1])\n","        next_frame = Image.fromarray(video_frames[i])\n","        score = calculate_clip_score(model,\n","                                     preprocess,\n","                                     first_data=pre_frame,\n","                                     second_data=next_frame,\n","                                     first_flag='img',\n","                                     second_flag='img').cpu().numpy()\n","        all_frame_scores += score\n","    return all_frame_scores / (len(video_frames) - 1)\n","\n","# GenVideo-Text clip\n","def compute_video_text_alignment(video_path, text):\n","    global model, preprocess\n","\n","    all_frame_scores = 0.\n","    video_frames = load_video(video_path, )[:16]\n","    assert len(video_frames) != 0, f'check video_path {video_path}!'\n","    for frame in video_frames:\n","        frame = Image.fromarray(frame)\n","        score = calculate_clip_score(model,\n","                                     preprocess,\n","                                     first_data=text,\n","                                     second_data=frame,\n","                                     first_flag='txt',\n","                                     second_flag='img').cpu().numpy()\n","        all_frame_scores += score\n","    return all_frame_scores / len(video_frames)\n","\n","# GenVideo-RefVideo clip (keyframes)\n","def compute_video_video_similarity(reference_video_path,\n","                                   video_path,\n","                                   keyframes=[0, 4, 8, 12, 15]):\n","    global model, preprocess\n","    print(\"Hitting this method\")\n","    print(f\"Processing reference video: {reference_video_path}\")\n","    reference_video = load_video(reference_video_path)\n","    print(f\"Reference video frames: {len(reference_video)}\")\n","\n","    try:\n","        print(f\"Processing video: {video_path}\")\n","        video_frames = load_video(video_path,\n","                                  size=reference_video[0].shape[:2])\n","        print(f\"Video frames: {len(video_frames)}\")\n","    except Exception as e:\n","        print(f\"Error loading video {video_path}: {str(e)}\")\n","        return {'clip': 0, 'state': 0}\n","\n","    all_frame_scores = 0.\n","    for idx, frames in enumerate(keyframes):\n","        try:\n","            print(f\"Processing keyframe {frames}\")\n","            im1 = Image.fromarray(np.array(reference_video[frames]))\n","            im2 = Image.fromarray(np.array(video_frames[frames]))\n","            score = calculate_clip_score(model,\n","                                         preprocess,\n","                                         first_data=im1,\n","                                         second_data=im2,\n","                                         first_flag='img',\n","                                         second_flag='img').cpu().numpy()\n","            all_frame_scores += score\n","            print(score)\n","        except IndexError as e:\n","            print(f\"IndexError at keyframe {frames}: {str(e)}\")\n","            continue\n","        except Exception as e:\n","            print(f\"Error processing keyframe {frames}: {str(e)}\")\n","            continue\n","\n","    if len(keyframes) > 0:\n","        all_frame_scores /= len(keyframes)\n","    else:\n","        print(\"No keyframes were successfully processed\")\n","\n","    return {'clip': all_frame_scores, 'state': 1}\n","\n","# MSE (First) and SSIM (First)\n","def compute_image_image_similarity(init_image_path, video_path):\n","    global model, preprocess\n","\n","    # all_frame_scores = 0.\n","    init_image = Image.fromarray(load_image(init_image_path))\n","    try:\n","        video_frames = load_video(video_path,\n","                                  size=init_image.size)  # size=(512, 904)\n","    except:\n","        print(video_path, ' Error !!')\n","        return {'MSE': 0, 'SSIM': 0, 'state': 0}\n","    init_image_np = np.array(init_image)\n","    video_frame_np = np.array(video_frames[0])  # or 10th frames\n","\n","    if init_image_np.shape == video_frame_np.shape:\n","        mse_value = mean_squared_error(init_image_np, video_frame_np)\n","        ssim_value = ssim(init_image_np,\n","                          video_frame_np,\n","                          multichannel=True,\n","                          win_size=7,\n","                          channel_axis=-1)\n","    else:\n","        print(\"Error: The images do not have the same dimensions.\", video_path)\n","        return {'MSE': 0, 'SSIM': 0, 'state': 0}\n","\n","    return {'MSE': mse_value, 'SSIM': ssim_value, 'state': 1}\n","\n","# Image-GenVideo clip\n","def compute_video_image_similarity(video_path, image_path):\n","    global model, preprocess\n","\n","    all_frame_scores = 0.\n","    image = Image.fromarray(load_image(image_path))\n","\n","    try:\n","        video_frames = load_video(video_path, size=image.size)\n","    except:\n","        print(video_path, ' Error !!')\n","        return {\"clip_per\": all_frame_scores / len(video_frames), \"state\": 0}\n","\n","    for frame in video_frames:\n","        frame = Image.fromarray(frame)\n","        score = calculate_clip_score(model,\n","                                     preprocess,\n","                                     first_data=image,\n","                                     second_data=frame,\n","                                     first_flag='img',\n","                                     second_flag='img').cpu().numpy()\n","        all_frame_scores += score\n","    return {\"clip_per\": all_frame_scores / len(video_frames), \"state\": 1}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyoMtMc_k6U6","executionInfo":{"status":"ok","timestamp":1721028872152,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"34c288c6-c98e-49e1-a217-705ab17df09f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/AIGCBench/eval.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"i0-8Nx7hxcnq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Got the videos from the google drive\n","\n","Google drive url is https://drive.google.com/drive/folders/1bpW3URL4S8kFG0vSlFSJ1zGEsidIvsjL?usp=drive_link"],"metadata":{"id":"scREv1TTbgOp"}},{"cell_type":"code","source":["import os\n","import glob\n","\n","# Path to your videos in Google Drive\n","video_folder = '/content/drive/MyDrive/Q-Align_data'\n","\n","# Get a list of all video files (assuming they're mp4, but adjust if needed)\n","video_path_list = sorted(glob.glob(os.path.join(video_folder, '*.mp4')))\n","\n","print(f\"Found {len(video_path_list)} videos.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlakL3SygL9M","executionInfo":{"status":"ok","timestamp":1720755955072,"user_tz":300,"elapsed":1091,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"00419209-1784-4f3b-93fc-9d9f7d39d47e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 30 videos.\n"]}]},{"cell_type":"code","source":["video_path_list[20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LcxBSh2wDCSe","executionInfo":{"status":"ok","timestamp":1720756129550,"user_tz":300,"elapsed":1085,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"6d5500ed-c310-4f4b-f4df-79ca6b753482"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Q-Align_data/Waterfalling_Decent.mp4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Use the first video as reference\n","from eval import compute_video_video_similarity\n","ref_video_path = video_path_list[0]\n","\n","sum_res = 0\n","cnt = 0\n","\n","for video_path in video_path_list[1:]:  # Skip the reference video\n","    res = compute_video_video_similarity(ref_video_path, video_path)\n","    sum_res += res['clip']\n","    cnt += res[\"state\"]\n","\n","if cnt > 0:\n","    print(f\"Average CLIP similarity: {sum_res / cnt}\")\n","else:\n","    print(\"No videos were processed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h97vlnP4gfAr","executionInfo":{"status":"ok","timestamp":1720743613620,"user_tz":300,"elapsed":43551,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"61f76290-de82-4b24-fc7b-ac9379055816"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/BeachwithProduct.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.72373533\n","Processing keyframe 4\n","0.76752216\n","Processing keyframe 8\n","0.7308178\n","Processing keyframe 12\n","0.69993424\n","Processing keyframe 15\n","0.7428811\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/ExtraHand.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.6805366\n","Processing keyframe 4\n","0.7240991\n","Processing keyframe 8\n","0.68420404\n","Processing keyframe 12\n","0.6692717\n","Processing keyframe 15\n","0.5334763\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/EyesDistorted.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.66286874\n","Processing keyframe 4\n","0.6791461\n","Processing keyframe 8\n","0.66778487\n","Processing keyframe 12\n","0.61907953\n","Processing keyframe 15\n","0.6541815\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/HeadAndShoulder.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.6156273\n","Processing keyframe 4\n","0.60087407\n","Processing keyframe 8\n","0.60009074\n","Processing keyframe 12\n","0.5591075\n","Processing keyframe 15\n","0.6120564\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Light1.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.42461237\n","Processing keyframe 4\n","0.36126503\n","Processing keyframe 8\n","0.4541341\n","Processing keyframe 12\n","0.40036935\n","Processing keyframe 15\n","0.5867507\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Plum.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.65347147\n","Processing keyframe 4\n","0.67033374\n","Processing keyframe 8\n","0.59456086\n","Processing keyframe 12\n","0.6267396\n","Processing keyframe 15\n","0.6205164\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/ShampooBottle.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.42461237\n","Processing keyframe 4\n","0.36126503\n","Processing keyframe 8\n","0.4541341\n","Processing keyframe 12\n","0.40036935\n","Processing keyframe 15\n","0.5867507\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Smile.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.70525706\n","Processing keyframe 4\n","0.7276187\n","Processing keyframe 8\n","0.6873108\n","Processing keyframe 12\n","0.6401305\n","Processing keyframe 15\n","0.7005737\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/SmillingFace.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.70085686\n","Processing keyframe 4\n","0.7154093\n","Processing keyframe 8\n","0.6808841\n","Processing keyframe 12\n","0.664285\n","Processing keyframe 15\n","0.7225627\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Sunrise.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.7870326\n","Processing keyframe 4\n","0.787938\n","Processing keyframe 8\n","0.7375937\n","Processing keyframe 12\n","0.72086006\n","Processing keyframe 15\n","0.7736804\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Water.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.72068083\n","Processing keyframe 4\n","0.74655634\n","Processing keyframe 8\n","0.69181454\n","Processing keyframe 12\n","0.665007\n","Processing keyframe 15\n","0.73178524\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/Waterfalling.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.6968407\n","Processing keyframe 4\n","0.755831\n","Processing keyframe 8\n","0.6906795\n","Processing keyframe 12\n","0.68049234\n","Processing keyframe 15\n","0.72623336\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/WomanEyeDistortion.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.72560406\n","Processing keyframe 4\n","0.7172596\n","Processing keyframe 8\n","0.69623804\n","Processing keyframe 12\n","0.6891638\n","Processing keyframe 15\n","0.569097\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/WomanShampoo.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.6860976\n","Processing keyframe 4\n","0.71021086\n","Processing keyframe 8\n","0.7010132\n","Processing keyframe 12\n","0.67611456\n","Processing keyframe 15\n","0.7206281\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/WomenwithProduct.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.72373533\n","Processing keyframe 4\n","0.76752216\n","Processing keyframe 8\n","0.7308178\n","Processing keyframe 12\n","0.69993424\n","Processing keyframe 15\n","0.7428811\n","Hitting this method\n","Processing reference video: /content/drive/MyDrive/Q-Align_data/Beach.mp4\n","Reference video frames: 16\n","Processing video: /content/drive/MyDrive/Q-Align_data/anatomy.mp4\n","Video frames: 16\n","Processing keyframe 0\n","0.75306404\n","Processing keyframe 4\n","0.7246826\n","Processing keyframe 8\n","0.68307436\n","Processing keyframe 12\n","0.6748793\n","Processing keyframe 15\n","0.70513517\n","Average CLIP similarity: 0.6600280947983267\n"]}]},{"cell_type":"code","source":["from eval import compute_video_text_alignment\n","compute_video_text_alignment(video_path_list[20],\"a dog playing with a ball\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfR1M72ImktC","executionInfo":{"status":"ok","timestamp":1720757659566,"user_tz":300,"elapsed":1200,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"56d93262-6058-4fc2-fa8d-742471d53e4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0777246505022049"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["from eval import compute_temporal_consistency\n","ref_video_path='/content/drive/MyDrive/Bad videos/object4.gif'\n","compute_temporal_consistency(ref_video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avsJz0S2U9CD","executionInfo":{"status":"ok","timestamp":1721029321929,"user_tz":-330,"elapsed":2358,"user":{"displayName":"Sumanth Anand","userId":"03826385155382849077"}},"outputId":"751c1a8d-b5b4-4bab-d8b0-32edd06e4540"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8395310878753662"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["compute_temporal_consistency(video_path_list[4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuTHhWzXVFPL","executionInfo":{"status":"ok","timestamp":1720746055971,"user_tz":300,"elapsed":2322,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"1a66d59e-a9ad-426f-8eda-d4c628b559c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9576089779535929"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["for video_path in video_path_list:  # Skip the reference video\n","    res = compute_temporal_consistency( video_path)\n","\n","    print(res)\n","    print(video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDqGoFd2daB0","executionInfo":{"status":"ok","timestamp":1720749115672,"user_tz":300,"elapsed":17329,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"522bf061-25fd-4ddb-e1af-2b14a371d2b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9812637448310852\n","/content/drive/MyDrive/Q-Align_data/Beach.mp4\n","0.9917521993319194\n","/content/drive/MyDrive/Q-Align_data/BeachwithProduct.mp4\n","0.9488892356554667\n","/content/drive/MyDrive/Q-Align_data/ExtraHand.mp4\n","0.9784121751785279\n","/content/drive/MyDrive/Q-Align_data/EyesDistorted.mp4\n","0.9612833460172018\n","/content/drive/MyDrive/Q-Align_data/FLowers_anatomy.mp4\n","0.9576089779535929\n","/content/drive/MyDrive/Q-Align_data/Face_Bad.mp4\n","0.9963203390439351\n","/content/drive/MyDrive/Q-Align_data/HeadAndShoulder.mp4\n","0.9350598951180776\n","/content/drive/MyDrive/Q-Align_data/Light1.mp4\n","0.9864495992660522\n","/content/drive/MyDrive/Q-Align_data/Plum.mp4\n","0.9350598951180776\n","/content/drive/MyDrive/Q-Align_data/ShampooBottle.mp4\n","0.9962668259938557\n","/content/drive/MyDrive/Q-Align_data/Smile.mp4\n","0.9983378330866496\n","/content/drive/MyDrive/Q-Align_data/SmillingFace.mp4\n","0.9703799525896708\n","/content/drive/MyDrive/Q-Align_data/SpiderMan_Bad.mp4\n","0.9907313903172811\n","/content/drive/MyDrive/Q-Align_data/Sunrise.mp4\n","0.9887410481770833\n","/content/drive/MyDrive/Q-Align_data/Water.mp4\n","0.9755588849385579\n","/content/drive/MyDrive/Q-Align_data/Waterfalling.mp4\n","0.9679451545079549\n","/content/drive/MyDrive/Q-Align_data/WomanEyeDistortion.mp4\n","0.9922633051872254\n","/content/drive/MyDrive/Q-Align_data/WomanShampoo.mp4\n","0.9917521993319194\n","/content/drive/MyDrive/Q-Align_data/WomenwithProduct.mp4\n","0.9736831545829773\n","/content/drive/MyDrive/Q-Align_data/anatomy.mp4\n"]}]},{"cell_type":"code","source":["for video_path in video_path_list:  # Skip the reference video\n","    res = compute_temporal_consistency( video_path)\n","\n","    print(res)\n","    print(video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmZHUCcTsTbd","executionInfo":{"status":"ok","timestamp":1720752472031,"user_tz":300,"elapsed":28949,"user":{"displayName":"Manisha Panda","userId":"06579866161168304639"}},"outputId":"87e2163a-0356-446b-b583-897f84c976ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.960559896628062\n","/content/drive/MyDrive/Q-Align_data/Anatomy_Bad.mp4\n","0.9812637448310852\n","/content/drive/MyDrive/Q-Align_data/Beach_Good.mp4\n","0.9917521993319194\n","/content/drive/MyDrive/Q-Align_data/BeachwithProduct_Good.mp4\n","0.9286013563474019\n","/content/drive/MyDrive/Q-Align_data/Car_Moving_Backlwards.mp4\n","0.9488892356554667\n","/content/drive/MyDrive/Q-Align_data/ExtraHand_Bad.mp4\n","0.9784121751785279\n","/content/drive/MyDrive/Q-Align_data/EyesDistorted_Bad.mp4\n","0.9612833460172018\n","/content/drive/MyDrive/Q-Align_data/FLowers_anatomy.mp4\n","0.9576089779535929\n","/content/drive/MyDrive/Q-Align_data/Face_Bad.mp4\n","0.9598904848098755\n","/content/drive/MyDrive/Q-Align_data/Girl_Good.mp4\n","0.9963203390439351\n","/content/drive/MyDrive/Q-Align_data/HeadAndShoulder_Good.mp4\n","0.9655776103337605\n","/content/drive/MyDrive/Q-Align_data/Human_Moving_Backward.mp4\n","0.9350598951180776\n","/content/drive/MyDrive/Q-Align_data/Light1_Bad.mp4\n","0.9980235616366069\n","/content/drive/MyDrive/Q-Align_data/Mask_good.mp4\n","0.9864495992660522\n","/content/drive/MyDrive/Q-Align_data/Plum_Good.mp4\n","0.9350598951180776\n","/content/drive/MyDrive/Q-Align_data/ShampooBottle_Bad.mp4\n","0.9962668259938557\n","/content/drive/MyDrive/Q-Align_data/Smile_Good.mp4\n","0.9983378330866496\n","/content/drive/MyDrive/Q-Align_data/SmillingFace_Good.mp4\n","0.9703799525896708\n","/content/drive/MyDrive/Q-Align_data/SpiderMan_Bad.mp4\n","0.9907313903172811\n","/content/drive/MyDrive/Q-Align_data/Sunrise_Good.mp4\n","0.9887410481770833\n","/content/drive/MyDrive/Q-Align_data/Water_Good.mp4\n","0.9755588849385579\n","/content/drive/MyDrive/Q-Align_data/Waterfalling_Decent.mp4\n","0.9679451545079549\n","/content/drive/MyDrive/Q-Align_data/WomanEyeDistortion_Bad.mp4\n","0.9922633051872254\n","/content/drive/MyDrive/Q-Align_data/WomanShampoo_Decent.mp4\n","0.9917521993319194\n","/content/drive/MyDrive/Q-Align_data/WomenwithProduct_Good.mp4\n","0.9826110084851583\n","/content/drive/MyDrive/Q-Align_data/WorkingOut_Bad.mp4\n","0.9736831545829773\n","/content/drive/MyDrive/Q-Align_data/anatomy_bad.mp4\n","0.9452077547709147\n","/content/drive/MyDrive/Q-Align_data/light_bad.mp4\n","0.9912035981814067\n","/content/drive/MyDrive/Q-Align_data/water_bad.mp4\n","0.9851893862088521\n","/content/drive/MyDrive/Q-Align_data/water_girl_bad.mp4\n","0.9465876738230388\n","/content/drive/MyDrive/Q-Align_data/women_Very_bad.mp4\n"]}]},{"cell_type":"markdown","source":["For the bad ones, the result is somewhere around 0.96 - 0.97. But some images where the limbs are not distorted or we don't see much of deformity in humans, the score is good, even if the image is bad. This is proabably because the human images are preserved."],"metadata":{"id":"BxlaPY39zO67"}}]}